{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import './../../loadenv.mjs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from '@langchain/community/document_loaders/web/cheerio'\n",
    "import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'\n",
    "import { MemoryVectorStore } from 'langchain/vectorstores/memory'\n",
    "import { getEmbeddings } from './../../utils.mjs'\n",
    "\n",
    "const urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "const docs = await Promise.all(\n",
    "    urls.map(url => new CheerioWebBaseLoader(url).load()),\n",
    ")\n",
    "const docsList = docs.flat()\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 500,\n",
    "    chunkOverlap: 250,\n",
    "})\n",
    "const docSplits = await textSplitter.splitDocuments(docsList)\n",
    "\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "    docSplits,\n",
    "    getEmbeddings()\n",
    ")\n",
    "const retriever = vectorStore.asRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from '@langchain/langgraph'\n",
    "import { type DocumentInterface } from '@langchain/core/documents'\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "    documents: Annotation<DocumentInterface[]>({\n",
    "        reducer: (x, y) => y ?? x ?? [],\n",
    "    }),\n",
    "    question: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x ?? '',\n",
    "    }),\n",
    "    generation: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x,\n",
    "        default: () => '',\n",
    "    }),\n",
    "    generationVQuestionGrade: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x,\n",
    "    }),\n",
    "    generationVDocumentsGrade: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x,\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from 'zod'\n",
    "import { ChatPromptTemplate } from '@langchain/core/prompts'\n",
    "import { pull } from 'langchain/hub'\n",
    "import { getModel } from './../../utils.mjs'\n",
    "import { StringOutputParser } from '@langchain/core/output_parsers'\n",
    "import type { RunnableConfig } from '@langchain/core/runnables'\n",
    "import { formatDocumentsAsString } from 'langchain/util/document'\n",
    "\n",
    "const model = getModel({\n",
    "    temperature: 0,\n",
    "})\n",
    "\n",
    "async function retrieve(\n",
    "    state: typeof GraphState.State,\n",
    "    config?: RunnableConfig,\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---RETRIEVE---')\n",
    "    const documents = await retriever\n",
    "        .withConfig({ runName: 'FetchRelevantDocuments'})\n",
    "        .invoke(state.question, config)\n",
    "    \n",
    "    return {\n",
    "        documents,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function generate(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---GENERATE---')\n",
    "\n",
    "    const prompt = await pull<ChatPromptTemplate>('rlm/rag-prompt')\n",
    "    const ragChain = prompt.pipe(model).pipe(new StringOutputParser())\n",
    "\n",
    "    const generation = await ragChain.invoke({\n",
    "        context: formatDocumentsAsString(state.documents),\n",
    "        question: state.question,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        generation,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function gradeDocuments(\n",
    "    state: typeof GraphState.State,\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---CHECK RELEVANCE---')\n",
    "    const llmWithTool = model.withStructuredOutput(\n",
    "        z.object({\n",
    "            binaryScore: z.enum(['yes', 'no']).describe(\"Relevance score 'yes' or 'no'\")\n",
    "        }).describe(\"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"),\n",
    "        {\n",
    "            name: 'grade',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are a grader assessing relevance of a retrieved document to a user question.\n",
    "  Here is the retrieved document:\n",
    "\n",
    "  {context}\n",
    "\n",
    "  Here is the user question: {question}\n",
    "\n",
    "  If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.`\n",
    "    )\n",
    "    const chain = prompt.pipe(llmWithTool)\n",
    "    const filteredDocs: Array<DocumentInterface> = []\n",
    "    for await (const doc of state.documents) {\n",
    "        const grade = await chain.invoke({\n",
    "            context: doc.pageContent,\n",
    "            question: state.question,\n",
    "        })\n",
    "        if (grade.binaryScore === 'yes') {\n",
    "            console.log('---GRADE: DOCUMENT RELEVANT---')\n",
    "            filteredDocs.push(doc)\n",
    "        } else {\n",
    "            console.log('---GRADE: DOCUMENT NOT RELEVANT---')\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        documents: filteredDocs,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function transformQuery(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---TRANSFORM QUERY---')\n",
    "\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are generating a question that is well optimized for semantic search retrieval.\n",
    "  Look at the input and try to reason about the underlying sematic intent / meaning.\n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Formulate an improved question: `\n",
    "    )\n",
    "    const chain = prompt.pipe(model).pipe(new StringOutputParser())\n",
    "    const betterQuestion = await chain.invoke({\n",
    "        question: state.question,\n",
    "    })\n",
    "    return {\n",
    "        question: betterQuestion,\n",
    "    }\n",
    "}\n",
    "\n",
    "function decideToGenerate(state: typeof GraphState.State) {\n",
    "    console.log('---DECIDE TO GENERATE---')\n",
    "\n",
    "    const filteredDocs = state.documents\n",
    "    if (filteredDocs.length === 0) {\n",
    "        console.log('---DECISION: TRANSFORM QUERY---')\n",
    "        return 'transformQuery'\n",
    "    }\n",
    "    console.log('---DECISION: GENERATE---')\n",
    "    return 'generate'\n",
    "}\n",
    "\n",
    "async function generateGenerationVDocumentsGrade(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---GENERATE GENERATION vs DOCUMENTS GRADE---')\n",
    "\n",
    "    const llmWithTool = model.withStructuredOutput(\n",
    "        z.object({\n",
    "            binaryScore: z.enum(['yes', 'no']).describe(\"Relevance score 'yes' or 'no'\")\n",
    "        }).describe(\"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"),\n",
    "        {\n",
    "            name: 'grade',\n",
    "        }\n",
    "    )\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "  Here are the facts:\n",
    "  \\n ------- \\n\n",
    "  {documents} \n",
    "  \\n ------- \\n\n",
    "  Here is the answer: {generation}\n",
    "  Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts.`\n",
    "    )\n",
    "    const chain = prompt.pipe(llmWithTool)\n",
    "\n",
    "    const score = await chain.invoke({\n",
    "        documents: formatDocumentsAsString(state.documents),\n",
    "        generation: state.generation,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        generationVDocumentsGrade: score.binaryScore,\n",
    "    }\n",
    "}\n",
    "\n",
    "function gradeGenerationVDocuments(state: typeof GraphState.State) {\n",
    "    console.log('---GRADE GENERATION vs DOCUMENTS---')\n",
    "\n",
    "    const grade = state.generationVDocumentsGrade\n",
    "    if (grade === 'yes') {\n",
    "        console.log('---DECISION: SUPPORTED, MOVED TO FINAL GRADE---')\n",
    "        return 'supported'\n",
    "    }\n",
    "    console.log('---DECISION: NOT SUPPORTED, GENERATE AGAIN---')\n",
    "    return 'not supported'\n",
    "}\n",
    "\n",
    "async function generateGenerationVQuestionGrade(\n",
    "    state: typeof GraphState.State,\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---GENERATE GENERATION vs QUESTION GRADE---')\n",
    "\n",
    "    const llmWithTool = model.withStructuredOutput(\n",
    "        z.object({\n",
    "            binaryScore: z.enum(['yes', 'no']).describe(\"Relevance score 'yes' or 'no'\")\n",
    "        }).describe(\"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"),\n",
    "        {\n",
    "            name: 'grade',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are a grader assessing whether an answer is useful to resolve a question.\n",
    "  Here is the answer:\n",
    "  \\n ------- \\n\n",
    "  {generation} \n",
    "  \\n ------- \\n\n",
    "  Here is the question: {question}\n",
    "  Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.`\n",
    "    )\n",
    "    const chain = prompt.pipe(llmWithTool)\n",
    "    const score = await chain.invoke({\n",
    "        question: state.question,\n",
    "        generation: state.generation,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        generationVQuestionGrade: score.binaryScore,\n",
    "    }\n",
    "}\n",
    "\n",
    "function gradeGenerationVQuestion(state: typeof GraphState.State) {\n",
    "    console.log('---GRADE GENERATION vs QUESTION---')\n",
    "\n",
    "    const grade = state.generationVQuestionGrade\n",
    "    if (grade === 'yes') {\n",
    "        console.log('---DECISION: USEFUL---')\n",
    "        return 'useful'\n",
    "    }\n",
    "    console.log('---DECISION: NOT USEFUL---')\n",
    "    return 'not useful'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from '@langchain/langgraph'\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "    .addNode('retrieve', retrieve)\n",
    "    .addNode('gradeDocuments', gradeDocuments)\n",
    "    .addNode('generate', generate)\n",
    "    .addNode(\n",
    "        'generateGenerationVDocumentsGrade',\n",
    "        generateGenerationVDocumentsGrade\n",
    "    )\n",
    "    .addNode('transformQuery', transformQuery)\n",
    "    .addNode(\n",
    "        'generateGenerationVQuestionGrade',\n",
    "        generateGenerationVQuestionGrade\n",
    "    )\n",
    "\n",
    "workflow.addEdge(START, 'retrieve')\n",
    "workflow.addEdge('retrieve', 'gradeDocuments')\n",
    "workflow.addConditionalEdges(\n",
    "    'gradeDocuments',\n",
    "    decideToGenerate,\n",
    "    {\n",
    "        transformQuery: 'transformQuery',\n",
    "        generate: 'generate',\n",
    "    }\n",
    ")\n",
    "workflow.addEdge('transformQuery', 'retrieve')\n",
    "workflow.addEdge('generate', 'generateGenerationVDocumentsGrade')\n",
    "workflow.addConditionalEdges(\n",
    "    'generateGenerationVDocumentsGrade',\n",
    "    gradeGenerationVDocuments,\n",
    "    {\n",
    "        supported: 'generateGenerationVQuestionGrade',\n",
    "        'not supported': 'generate',\n",
    "    }\n",
    ")\n",
    "workflow.addConditionalEdges(\n",
    "    'generateGenerationVQuestionGrade',\n",
    "    gradeGenerationVQuestion,\n",
    "    {\n",
    "        useful: END,\n",
    "        'not useful': 'transformQuery',\n",
    "    }\n",
    ")\n",
    "\n",
    "const app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const inputs = {\n",
    "    question: 'Explain how the different types of agent memory work.'\n",
    "}\n",
    "const config = { recursionLimit: 50 }\n",
    "\n",
    "const prettifyOutput = (output: Record<string, any>) => {\n",
    "    const key = Object.keys(output)[0]\n",
    "    const value = output[key]\n",
    "    console.log(`Node: '${key}'`)\n",
    "    if (key === 'retrieve' && 'documents' in value) {\n",
    "        console.log(`Retrieved ${value.documents.length} documents.`)\n",
    "    } else if (key === 'gradeDocuments' && 'documents' in value) {\n",
    "        console.log(`Graded documents. Found ${value.documents.length} relevant document(s).`)\n",
    "    } else {\n",
    "        console.dir(value, { depth: null })\n",
    "    }\n",
    "}\n",
    "\n",
    "for await (const output of await app.stream(inputs, config)) {\n",
    "    prettifyOutput(output)\n",
    "    console.log('\\n---ITERATION END---\\n')\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
