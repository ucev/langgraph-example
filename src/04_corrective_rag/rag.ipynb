{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import './../../loadenv.mjs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from '@langchain/community/document_loaders/web/cheerio'\n",
    "import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'\n",
    "import { MemoryVectorStore } from 'langchain/vectorstores/memory'\n",
    "import { getEmbeddings } from './../../utils.mjs'\n",
    "\n",
    "const urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "const docs = await Promise.all(\n",
    "    urls.map(url => new CheerioWebBaseLoader(url).load()),\n",
    ")\n",
    "const docsList = docs.flat()\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 250,\n",
    "    chunkOverlap: 0,\n",
    "})\n",
    "const docSplits = await textSplitter.splitDocuments(docsList)\n",
    "\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "    docSplits,\n",
    "    getEmbeddings(),\n",
    ")\n",
    "const retriever = vectorStore.asRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from '@langchain/langgraph'\n",
    "import { DocumentInterface } from '@langchain/core/documents'\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "    documents: Annotation<DocumentInterface[]>({\n",
    "        reducer: (x, y) => y ?? x ?? [],\n",
    "    }),\n",
    "    question: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x ?? '',\n",
    "    }),\n",
    "    generation: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x,\n",
    "    }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from '@langchain/community/tools/tavily_search'\n",
    "import { Document } from '@langchain/core/documents'\n",
    "import { z } from 'zod'\n",
    "import { ChatPromptTemplate } from '@langchain/core/prompts'\n",
    "import { pull } from 'langchain/hub'\n",
    "import { getModel } from './../../utils.mjs'\n",
    "import { StringOutputParser } from '@langchain/core/output_parsers'\n",
    "import { formatDocumentsAsString } from 'langchain/util/document'\n",
    "\n",
    "const model = getModel({\n",
    "    temperature: 0,\n",
    "})\n",
    "\n",
    "async function retrieve(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---RETRIEVE---')\n",
    "\n",
    "    const documents = await retriever\n",
    "        .withConfig({ runName: 'FetchRelevantDocuments'})\n",
    "        .invoke(state.question)\n",
    "    \n",
    "    return {\n",
    "        documents,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function generate(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---GENERATE---')\n",
    "\n",
    "    const prompt = await pull<ChatPromptTemplate>('rlm/rag-prompt')\n",
    "    const ragChain = prompt.pipe(model).pipe(new StringOutputParser())\n",
    "\n",
    "    const generation = await ragChain.invoke({\n",
    "        context: formatDocumentsAsString(state.documents),\n",
    "        question: state.question,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        generation,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function gradeDocuments(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---CHECK RELEVANCE---')\n",
    "\n",
    "    const llmWithTool = model.withStructuredOutput(\n",
    "        z.object({\n",
    "            binaryScore: z.enum(['yes', 'no']).describe('Relevance score \"yes\" or \"no\"')\n",
    "        }).describe(\"Grade the relevance of the retrieved documents to the question. Either 'yes' or 'no'.\"),\n",
    "        {\n",
    "            name: 'grade',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are a grader assessing relevance of a retrieved document to a user question.\n",
    "  Here is the retrieved document:\n",
    "\n",
    "  {context}\n",
    "\n",
    "  Here is the user question: {question}\n",
    "\n",
    "  If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.`\n",
    "    )\n",
    "    const chain = prompt.pipe(llmWithTool)\n",
    "\n",
    "    const filteredDocs: Array<DocumentInterface> = []\n",
    "    for await (const doc of state.documents) {\n",
    "        const grade = await chain.invoke({\n",
    "            context: doc.pageContent,\n",
    "            question: state.question,\n",
    "        })\n",
    "        if (grade.binaryScore === 'yes') {\n",
    "            console.log('---GRADE: DOCUMENT RELEVANT---')\n",
    "            filteredDocs.push(doc)\n",
    "        } else {\n",
    "            console.log('---GRADE: DOCUMENT NOT RELEVANT---')\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        documents: filteredDocs,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function transformQuery(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---TRANSFORM QUERY---')\n",
    "\n",
    "    const prompt = ChatPromptTemplate.fromTemplate(\n",
    "        `You are generating a question that is well optimized for semantic search retrieval.\n",
    "  Look at the input and try to reason about the underlying sematic intent / meaning.\n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Formulate an improved question: `\n",
    "    )\n",
    "    const chain = prompt.pipe(model).pipe(new StringOutputParser())\n",
    "    const betterQuestion = await chain.invoke({\n",
    "        question: state.question,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        question: betterQuestion,\n",
    "    }\n",
    "}\n",
    "\n",
    "async function webSearch(\n",
    "    state: typeof GraphState.State\n",
    "): Promise<Partial<typeof GraphState.State>> {\n",
    "    console.log('---WEB SEARCH---')\n",
    "\n",
    "    const tool = new TavilySearchResults()\n",
    "    const docs = await tool.invoke({\n",
    "        query: state.question,\n",
    "    })\n",
    "    const webResults = new Document({ pageContent: docs })\n",
    "    const newDocuments = state.documents.concat(webResults)\n",
    "\n",
    "    return {\n",
    "        documents: newDocuments,\n",
    "    }\n",
    "}\n",
    "\n",
    "function decideToGenerate(state: typeof GraphState.State) {\n",
    "    console.log('---DECIDE TO GENERATE---')\n",
    "\n",
    "    const filteredDocs = state.documents\n",
    "    if (filteredDocs.length === 0) {\n",
    "        console.log('---DECISION: TRANSFORM QUERY---')\n",
    "        return 'transformQuery'\n",
    "    }\n",
    "    console.log('---DECISION: GENERATE---')\n",
    "    return 'generate'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from '@langchain/langgraph'\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "    .addNode('retrieve', retrieve)\n",
    "    .addNode('gradeDocuments', gradeDocuments)\n",
    "    .addNode('generate', generate)\n",
    "    .addNode('transformQuery', transformQuery)\n",
    "    .addNode('webSearch', webSearch)\n",
    "\n",
    "workflow.addEdge(START, 'retrieve')\n",
    "workflow.addEdge('retrieve', 'gradeDocuments')\n",
    "workflow.addConditionalEdges(\n",
    "    'gradeDocuments',\n",
    "    decideToGenerate,\n",
    ")\n",
    "workflow.addEdge('transformQuery', 'webSearch')\n",
    "workflow.addEdge('webSearch', 'generate')\n",
    "workflow.addEdge('generate', END)\n",
    "\n",
    "const app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { printGraph } from './../../utils.mjs'\n",
    "await printGraph(app.getGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const inputs = {\n",
    "    question: 'Explain how the different types of agent memory work.'\n",
    "}\n",
    "const config = { recursionLimit: 50 }\n",
    "let finalGeneration\n",
    "for await (const output of await app.stream(inputs, config)) {\n",
    "    for (const [key, value] of Object.entries(output)) {\n",
    "        console.log(`Node: ${key}`)\n",
    "        finalGeneration = value\n",
    "    }\n",
    "    console.log('\\n---\\n')\n",
    "}\n",
    "\n",
    "console.log(JSON.stringify(finalGeneration, null, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
