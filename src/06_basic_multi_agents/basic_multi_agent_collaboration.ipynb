{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import './../../loadenv.mjs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'\n",
    "import { StructuredTool } from '@langchain/core/tools'\n",
    "import { convertToOpenAITool } from '@langchain/core/utils/function_calling'\n",
    "import { Runnable } from '@langchain/core/runnables'\n",
    "import { getModel } from './../../utils.mjs'\n",
    "import { ChatOpenAI } from '@langchain/openai'\n",
    "\n",
    "async function createAgent({\n",
    "    llm,\n",
    "    tools,\n",
    "    systemMessage\n",
    "}: {\n",
    "    llm: ChatOpenAI;\n",
    "    tools: StructuredTool[];\n",
    "    systemMessage: string;\n",
    "}): Promise<Runnable> {\n",
    "    const toolNames = tools.map(tool => tool.name).join(', ')\n",
    "    const formattedTools = tools.map(t => convertToOpenAITool(t))\n",
    "\n",
    "    let prompt = ChatPromptTemplate.fromMessages([\n",
    "        [\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\" +\n",
    "            \" Use the provided tools to progress towards answering the question.\" +\n",
    "            \" If you are unable to fully answer, that's OK, another assistant with different tools \" +\n",
    "            \" will help where you left off. Execute what you can to make progress.\" +\n",
    "            // \" If you or any of the other assistants have the final answer or deliverable,\" +\n",
    "            // \" prefix your response with FINAL ANSWER so the team knows to stop.\" +\n",
    "            /// 根据豆包的功能，对提示词进行修改\n",
    "            \" Only if you have all ready displayed a bar chart to user, \" +\n",
    "            \" can you prefix your response with FINAL ANSWER so the team knows to stop.\" +\n",
    "\n",
    "            \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "        ],\n",
    "        new MessagesPlaceholder(\"messages\"),\n",
    "    ])\n",
    "    prompt = await prompt.partial({\n",
    "        system_message: systemMessage,\n",
    "        tool_names: toolNames,\n",
    "    })\n",
    "\n",
    "    return prompt.pipe(llm.bind({ tools: formattedTools }))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from '@langchain/core/messages'\n",
    "import { Annotation } from '@langchain/langgraph'\n",
    "\n",
    "const AgentState = Annotation.Root({\n",
    "    messages: Annotation<BaseMessage[]>({\n",
    "        reducer: (x, y) => x.concat(y),\n",
    "    }),\n",
    "    sender: Annotation<string>({\n",
    "        reducer: (x, y) => y ?? x ?? 'user',\n",
    "        default: () => 'user',\n",
    "    }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from '@langchain/community/tools/tavily_search'\n",
    "import { chartTool } from './../../utils.mjs'\n",
    "\n",
    "const tavilyTool = new TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define agent nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HumanMessage } from '@langchain/core/messages'\n",
    "import type { RunnableConfig } from '@langchain/core/runnables'\n",
    "\n",
    "async function runAgentNode(props: {\n",
    "    state: typeof AgentState.State;\n",
    "    agent: Runnable;\n",
    "    name: string;\n",
    "    config?: RunnableConfig;\n",
    "}) {\n",
    "    const { state, agent, name, config } = props\n",
    "    let result = await agent.invoke(state, config)\n",
    "    if (!result?.tool_calls || result.tool_calls.length === 0) {\n",
    "        result = new HumanMessage({ ...result, name, })\n",
    "    }\n",
    "    return {\n",
    "        messages: [result],\n",
    "        sender: name,\n",
    "    }\n",
    "}\n",
    "\n",
    "const llm = getModel()\n",
    "\n",
    "const researchAgent = await createAgent({\n",
    "    llm,\n",
    "    tools: [tavilyTool],\n",
    "    systemMessage: 'You should provide accurate data for the chart generator to use.\\nLet\\'s think step by step.'\n",
    "})\n",
    "\n",
    "async function researchNode(\n",
    "    state: typeof AgentState.State,\n",
    "    config?: RunnableConfig,\n",
    ") {\n",
    "    return runAgentNode({\n",
    "        state,\n",
    "        agent: researchAgent,\n",
    "        name: 'Researcher',\n",
    "        config,\n",
    "    })\n",
    "}\n",
    "\n",
    "const chartAgent = await createAgent({\n",
    "    llm,\n",
    "    tools: [chartTool],\n",
    "    systemMessage: 'Any charts you display will be visible by the user.'\n",
    "})\n",
    "\n",
    "async function chartNode(state: typeof AgentState.State) {\n",
    "    return runAgentNode({\n",
    "        state,\n",
    "        agent: chartAgent,\n",
    "        name: 'ChartGenerator',\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// const researchResults = await researchNode({\n",
    "//     messages: [new HumanMessage('Research the US primaries in 2024')],\n",
    "//     sender: 'User',\n",
    "// })\n",
    "\n",
    "// researchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define tool node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolNode } from '@langchain/langgraph/prebuilt'\n",
    "\n",
    "const tools = [tavilyTool, chartTool]\n",
    "const toolNode = new ToolNode<typeof AgentState.State>(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// await toolNode.invoke(researchResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define edge logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AIMessage } from '@langchain/core/messages'\n",
    "\n",
    "function router(state: typeof AgentState.State) {\n",
    "    const messages = state.messages\n",
    "    const lastMessage = messages[messages.length - 1] as AIMessage\n",
    "    if (lastMessage?.tool_calls && lastMessage.tool_calls.length > 0) {\n",
    "        return 'call_tool'\n",
    "    }\n",
    "    if (typeof lastMessage.content === 'string' && lastMessage.content.startsWith('FINAL ANSWER')) {\n",
    "        return 'end'\n",
    "    }\n",
    "    return 'continue'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, START, StateGraph } from '@langchain/langgraph'\n",
    "\n",
    "const workflow = new StateGraph(AgentState)\n",
    "    .addNode('Researcher', researchNode)\n",
    "    .addNode('ChartGenerator', chartNode)\n",
    "    .addNode('call_tool', toolNode)\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "    'Researcher',\n",
    "    router,\n",
    "    {\n",
    "        continue: 'ChartGenerator',\n",
    "        call_tool: 'call_tool',\n",
    "        end: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "    'ChartGenerator',\n",
    "    router,\n",
    "    {\n",
    "        continue: 'Researcher',\n",
    "        call_tool: 'call_tool',\n",
    "        end: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "    'call_tool',\n",
    "    x => x.sender,\n",
    "    {\n",
    "        Researcher: 'Researcher',\n",
    "        ChartGenerator: 'ChartGenerator',\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.addEdge(START, 'Researcher')\n",
    "const graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { printGraph } from './../../utils.mjs'\n",
    "await printGraph(graph.getGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const streamResults = await graph.stream(\n",
    "    {\n",
    "        messages: [\n",
    "            new HumanMessage({\n",
    "                content: 'Generate a bar chart of the US gdp over the past 3 years.'\n",
    "            }),\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        recursionLimit: 150,\n",
    "    }\n",
    ")\n",
    "\n",
    "const prettifyOutput = (output: Record<string, any>) => {\n",
    "    const keys = Object.keys(output)\n",
    "    const firstItem = output[keys[0]]\n",
    "\n",
    "    if ('messages' in firstItem && Array.isArray(firstItem.messages)) {\n",
    "        const lastMessage = firstItem.messages[firstItem.messages.length - 1]\n",
    "        console.dir({\n",
    "            type: lastMessage._getType(),\n",
    "            content: lastMessage.content,\n",
    "            tool_calls: lastMessage.tool_calls,\n",
    "        })\n",
    "    }\n",
    "    if ('sender' in firstItem) {\n",
    "        console.log({\n",
    "            sender: firstItem.sender,\n",
    "        })\n",
    "    }\n",
    "}\n",
    "\n",
    "for await (const output of await streamResults) {\n",
    "    if (!output?.__end__) {\n",
    "        prettifyOutput(output)\n",
    "        console.log('----')\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
