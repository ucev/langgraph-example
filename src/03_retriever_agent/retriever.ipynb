{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!多了`agent->generate`、`agent->gradeDocuments`、`agent->rewrite`三条边，不知道怎么出来的。需要研究一下!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import './../../loadenv.mjs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CheerioWebBaseLoader } from '@langchain/community/document_loaders/web/cheerio'\n",
    "import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'\n",
    "import { MemoryVectorStore } from 'langchain/vectorstores/memory'\n",
    "import { getEmbeddings } from './../../utils.mjs'\n",
    "\n",
    "const urls = [\n",
    "    'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
    "    'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/',\n",
    "    'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\n",
    "]\n",
    "\n",
    "const docs = await Promise.all(\n",
    "    urls.map(url => new CheerioWebBaseLoader(url).load()),\n",
    ")\n",
    "const docsList = docs.flat()\n",
    "\n",
    "const textSplitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 500,\n",
    "    chunkOverlap: 50,\n",
    "})\n",
    "const docSplits = await textSplitter.splitDocuments(docsList)\n",
    "\n",
    "const vectorStore = await MemoryVectorStore.fromDocuments(\n",
    "    docSplits,\n",
    "    getEmbeddings(),\n",
    ")\n",
    "\n",
    "const retriever = vectorStore.asRetriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docSplits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Annotation } from '@langchain/langgraph'\n",
    "import { BaseMessage } from '@langchain/core/messages'\n",
    "\n",
    "const GraphState = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createRetrieverTool } from 'langchain/tools/retriever'\n",
    "import { ToolNode } from '@langchain/langgraph/prebuilt'\n",
    "\n",
    "const tool = createRetrieverTool(\n",
    "  retriever,\n",
    "  {\n",
    "    name: 'retrieve_blog_posts',\n",
    "    description: 'Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.'\n",
    "\n",
    "  }\n",
    ")\n",
    "\n",
    "const tools = [tool]\n",
    "\n",
    "const toolNode = new ToolNode<typeof GraphState.State>(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END } from '@langchain/langgraph'\n",
    "import { pull } from 'langchain/hub'\n",
    "import { z } from 'zod'\n",
    "import { ChatPromptTemplate } from '@langchain/core/prompts'\n",
    "import { getModel } from './../../utils.mjs'\n",
    "import { AIMessage } from '@langchain/core/messages'\n",
    "\n",
    "function shouldRetrieve(state: typeof GraphState.State): string {\n",
    "  const { messages } = state\n",
    "  console.log('---DECIDE TO RETRIEVE---')\n",
    "  const lastMessage = messages[messages.length - 1]\n",
    "\n",
    "  if ('tool_calls' in lastMessage && Array.isArray(lastMessage.tool_calls) && lastMessage.tool_calls.length) {\n",
    "    console.log('---DECISION: RETRIEVE---')\n",
    "    return 'retrieve'\n",
    "  }\n",
    "  return END\n",
    "}\n",
    "\n",
    "async function gradeDocuments(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log('---GET RELEVANCE---')\n",
    "\n",
    "  const { messages } = state\n",
    "  const tool = {\n",
    "    name: 'give_relevance_score',\n",
    "    description: 'Give a relevance score to the retrieved documents.',\n",
    "    schema: z.object({\n",
    "      binaryScore: z.string().describe(\"Relevance score 'yes' or 'no'\")\n",
    "    })\n",
    "  }\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `You are a grader assessing relevance of retrieved docs to a user question.\n",
    "Here are the retrieved docs:\n",
    "\\n ------- \\n\n",
    "{context} \n",
    "\\n ------- \\n\n",
    "Here is the user question: {question}\n",
    "If the content of the docs are relevant to the users question, score them as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the docs are relevant to the question.\n",
    "Yes: The docs are relevant to the question.\n",
    "No: The docs are not relevant to the question.`,\n",
    "  )\n",
    "\n",
    "  const model = getModel({\n",
    "    temperature: 0,\n",
    "  }).bindTools([tool], {\n",
    "    tool_choice: tool.name,\n",
    "  })\n",
    "\n",
    "  const chain = prompt.pipe(model)\n",
    "\n",
    "  const lastMessage = messages[messages.length - 1]\n",
    "  const score = await chain.invoke({\n",
    "    question: messages[0].content as string,\n",
    "    context: lastMessage.content as string,\n",
    "  })\n",
    "\n",
    "  return {\n",
    "    messages: [score],\n",
    "  }\n",
    "}\n",
    "\n",
    "function checkRelevance(state: typeof GraphState.State): string {\n",
    "  console.log('---CHECK RELEVANCE---')\n",
    "  const { messages } = state\n",
    "  const lastMessage = messages[messages.length - 1]\n",
    "  if (!('tool_calls' in lastMessage)) {\n",
    "    throw new Error(\"The 'checkRelevance' node requires the most recent message to contain tool calls.\")\n",
    "  }\n",
    "  const toolCalls = (lastMessage as AIMessage).tool_calls\n",
    "  if (!toolCalls || !toolCalls.length) {\n",
    "    throw new Error('Last message was not a function message')\n",
    "  }\n",
    "  if (toolCalls[0].args.binaryScore === 'yes') {\n",
    "    console.log('---DECISION: DOCS RELEVANT---')\n",
    "    return 'yes'\n",
    "  }\n",
    "  console.log('---DECISION: DOCS NOT RELEVANT---')\n",
    "  return 'no'\n",
    "}\n",
    "\n",
    "async function agent(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log('---CALL AGENT---')\n",
    "  const { messages } = state\n",
    "  const filteredMessages = messages.filter((message) => {\n",
    "    if ('tool_calls' in message && Array.isArray(message.tool_calls) && message.tool_calls.length > 0) {\n",
    "      return message.tool_calls[0].name !== 'give_relevance_score'\n",
    "    }\n",
    "    return true\n",
    "  })\n",
    "  console.log(filteredMessages)\n",
    "  const model = getModel({\n",
    "    temperature: 0,\n",
    "    streaming: true\n",
    "  }).bindTools(tools)\n",
    "  const response = await model.invoke(filteredMessages)\n",
    "  return {\n",
    "    messages: [response],\n",
    "  }\n",
    "}\n",
    "\n",
    "async function rewrite(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log('---TRANSFORM QUERY---')\n",
    "  const { messages } = state\n",
    "  const question = messages[0].content as string\n",
    "  const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    `Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "  Here is the initial question:\n",
    "  \\n ------- \\n\n",
    "  {question} \n",
    "  \\n ------- \\n\n",
    "  Formulate an improved question:`,\n",
    "  )\n",
    "  const model = getModel({\n",
    "    temperature: 0,\n",
    "    streaming: true,\n",
    "  })\n",
    "  const response = await prompt.pipe(model).invoke({ question })\n",
    "  return {\n",
    "    messages: [response],\n",
    "  }\n",
    "}\n",
    "\n",
    "async function generate(state: typeof GraphState.State): Promise<Partial<typeof GraphState.State>> {\n",
    "  console.log('---GENERATE---')\n",
    "  const { messages } = state\n",
    "  const question = messages[0].content as string\n",
    "  const lastToolMessage = messages.slice().reverse().find(msg => msg._getType() === 'tool')\n",
    "  if (!lastToolMessage) {\n",
    "    throw new Error('No tool message found in the conversation history')\n",
    "  }\n",
    "  const docs = lastToolMessage.content as string\n",
    "  const prompt = await pull<ChatPromptTemplate>('rlm/rag-prompt')\n",
    "  const llm = getModel({\n",
    "    temperature: 0,\n",
    "    streaming: true,\n",
    "  })\n",
    "\n",
    "  const ragChain = prompt.pipe(llm)\n",
    "\n",
    "  const response = await ragChain.invoke({\n",
    "    context: docs,\n",
    "    question\n",
    "  })\n",
    "\n",
    "  return {\n",
    "    messages: [response],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StateGraph } from '@langchain/langgraph'\n",
    "\n",
    "const workflow = new StateGraph(GraphState)\n",
    "  .addNode('agent', agent)\n",
    "  .addNode('retrieve', toolNode)\n",
    "  .addNode('gradeDocuments', gradeDocuments)\n",
    "  .addNode('rewrite', rewrite)\n",
    "  .addNode('generate', generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { START } from '@langchain/langgraph'\n",
    "\n",
    "workflow.addEdge(START, 'agent')\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  'agent',\n",
    "  shouldRetrieve,\n",
    ")\n",
    "\n",
    "workflow.addEdge('retrieve', 'gradeDocuments')\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  'gradeDocuments',\n",
    "  checkRelevance,\n",
    "  {\n",
    "    yes: 'generate',\n",
    "    no: 'rewrite',\n",
    "  }\n",
    ")\n",
    "\n",
    "workflow.addEdge('generate', END)\n",
    "workflow.addEdge('rewrite', 'agent')\n",
    "\n",
    "const app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// const graph = app.getGraph()\n",
    "// graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { printGraph } from './../../utils.mjs'\n",
    "await printGraph(app.getGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HumanMessage } from '@langchain/core/messages'\n",
    "\n",
    "const inputs = {\n",
    "    messages: [\n",
    "        new HumanMessage(\n",
    "            \"What are the types of agent memory based on Lilian Weng's blog post?\",\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "let finalState;\n",
    "for await (const output of await app.stream(inputs)) {\n",
    "    for (const [key, value] of Object.entries(output)) {\n",
    "        const lastMsg = output[key].messages[output[key].messages.length - 1]\n",
    "        console.log(`Output from node: '${key}'`)\n",
    "        console.dir({\n",
    "            type: lastMsg._getType(),\n",
    "            content: lastMsg.content,\n",
    "            tool_calls: lastMsg.tool_calls,\n",
    "        }, { depth: null })\n",
    "        console.log('---\\n')\n",
    "        finalState = value\n",
    "    }\n",
    "}\n",
    "\n",
    "console.log(JSON.stringify(finalState, null, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
